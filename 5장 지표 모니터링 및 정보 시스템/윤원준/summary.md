# 5장: 지표 모니터링 및 경보 시스템 설계

**설계의 트레이드오프, 면접 질문, 심화 토론**을 위주로 진행하기 위한 정리 자료입니다.

---

## 1. 핵심 요약
* **목표**: 대규모 인프라(서버 10만 대)의 운영 지표를 수집, 저장, 시각화하고 이상 징후 발생 시 경보를 보내는 시스템 설계.
* **핵심 컴포넌트**:
    * **데이터 수집**: Pull(Prometheus) 또는 Push(CloudWatch) 모델로 지표 수집.
    * **데이터 전송**: Kafka를 버퍼로 사용하여 수집과 처리 계층을 분리하고 안정성 확보.
    * **데이터 저장**: 시계열 데이터베이스(TSDB)인 InfluxDB, Prometheus 등을 사용하여 대량의 시계열 데이터를 효율적으로 저장 및 쿼리.
    * **경보**: 설정된 규칙에 따라 이상 징후를 감지하고 이메일, PagerDuty 등으로 알림 전송.
    * **시각화**: Grafana 등 전문 도구를 사용하여 데이터를 차트와 대시보드로 시각화.
* **주요 특징**:
    * **시계열 데이터(Time-Series Data)**: 모든 지표는 타임스탬프와 값의 쌍으로 구성.
    * **다운샘플링(Downsampling)**: 데이터 보관 기간에 따라 해상도를 낮춰(예: 1초 -> 1분 -> 1시간) 저장 공간 최적화.
    * **고가용성 및 확장성**: Kafka와 분산 TSDB를 통해 높은 처리량과 장애 내구성 확보.

---

## 2. 핵심 트레이드오프 및 설계 논의 (Trade-offs)
설계 선택지 사이의 장단점을 비교하고, 상황에 맞는 최적의 선택을 논의합니다.

### 2.1. 지표 수집 방식: Pull vs Push
| 구분 | **Pull 모델** (수집기가 데이터를 당겨옴) | **Push 모델** (애플리케이션이 데이터를 밀어줌) |
| :--- | :--- | :--- |
| **디버깅** | **쉬움** (`/metrics` 엔드포인트 직접 확인 가능) | 어려움 (데이터가 오지 않을 때 원인 파악 힘듦) |
| **상태 진단** | **쉬움** (Pull 실패 시 즉시 장애 감지) | 어려움 (네트워크 문제인지 서버 문제인지 불분명) |
| **단기 실행 작업** | 어려움 (수집 전에 작업이 종료될 수 있음) | **유리함** (종료 직전에 데이터 Push 가능) |
| **네트워크 구성** | 복잡함 (방화벽 등 모든 엔드포인트 접근 허용 필요) | **단순함** (수집기 주소만 알면 됨) |
| **대표 시스템** | **Prometheus** | **Amazon CloudWatch, Graphite** |

> **논의 포인트**: 
> * "컨테이너 환경(Kubernetes)처럼 서비스 엔드포인트가 동적으로 변하는 상황에서는?" (서비스 탐색(Service Discovery)이 결합된 Pull 모델이 유리)
> * "방화벽으로 격리된 여러 데이터센터를 모니터링해야 한다면?" (Push 모델이 구성이 더 간단할 수 있음)

### 2.2. 데이터 집계 시점: 수집 시 vs 저장 시 vs 조회 시
| 시점 | **수집 시 (Agent)** | **저장 시 (Stream Processor)** | **조회 시 (Query-time)** |
| :--- | :--- | :--- | :--- |
| **장점** | 네트워크 트래픽 감소 | 저장 공간 대폭 절약 | **원본 데이터 보존, 유연한 분석 가능** |
| **단점** | 복잡한 집계 로직 구현 어려움 | 늦게 도착하는 데이터 처리 곤란 | **조회 속도 느림, 쿼리 부하 높음** |
| **적합성** | 간단한 사전 집계 (예: 분당 요청 수) | 대규모 데이터 파이프라인 | **대부분의 최신 모니터링 시스템** |

> **논의 포인트**: 
> * "실시간 집계와 원본 데이터 보존, 두 마리 토끼를 다 잡으려면?" (조회 시 집계를 기본으로 하되, 자주 쓰는 쿼리는 미리 집계하여 Materialized View처럼 만들어두는 하이브리드 방식)

---

## 3. 면접 예상 질문 (Interview Q&A)
면접관이 꼬리를 물고 들어올 수 있는 질문들과 모범 답변 키워드입니다.

### Q1. "왜 일반 RDBMS나 NoSQL 대신 시계열 데이터베이스(TSDB)를 사용해야 하나요?"
* **핵심 키워드**: `쓰기 성능 최적화`, `높은 압축률`, `시간 기반 함수`
* **답변 논리**:
    * TSDB는 **시간 순서에 따른 대량의 쓰기(Append-only)**에 최적화되어 있습니다.
    * 델타 인코딩 등 시계열 데이터 특화 **압축 알고리즘**을 사용하여 저장 공간을 획기적으로 줄입니다.
    * 이동 평균(Moving Average), 시간 버킷(Time Bucketing) 등 **시간 기반 분석 함수**를 내장하여 복잡한 쿼리를 쉽게 처리할 수 있습니다.

### Q2. "수집해야 할 지표가 너무 많아지면(High Cardinality) 어떤 문제가 발생하고 어떻게 해결하나요?"
* **핵심 키워드**: `카디널리티 폭발`, `인덱스 크기 증가`, `레이블(Label) 설계`
* **답변 논리**:
    * 레이블(Label)의 조합이 많아지면(예: 사용자 ID, 요청 ID를 레이블로 사용) 생성되는 시계열의 수가 기하급수적으로 늘어나 **카디널리티 폭발**이 발생합니다.
    * 이는 TSDB의 **인덱스 크기를 비대하게 만들어** 메모리 부족과 쿼리 성능 저하를 유발합니다.
    * **해결책**: 고유값이 많은 정보는 레이블이 아닌 로그(Log)로 기록하고, 레이블은 서버 역할, 환경 등 제한된 값의 조합으로만 사용해야 합니다.

### Q3. "짧은 시간 동안 같은 경보가 수백 개씩 울리는 '알림 폭풍(Alert Storm)'은 어떻게 방지하나요?"
* **핵심 키워드**: `그룹핑(Grouping)`, `중복 제거(Deduplication)`, `억제(Inhibition)`
* **답변 논리**:
    * **그룹핑**: 같은 클러스터에서 발생한 CPU 경보들을 하나의 알림으로 묶어 보냅니다.
    * **중복 제거**: 이미 발생한 경보가 해결되기 전까지는 동일한 경보를 다시 보내지 않습니다.
    * **억제**: 더 심각한 경보(예: '데이터센터 다운')가 발생하면, 그 하위 경보(예: '개별 서버 응답 없음')는 보내지 않도록 억제 규칙을 설정합니다.

---

## 4. 심화 토론 주제 (Deep Dive Discussion)
단순한 구현을 넘어 시스템의 확장성과 엣지 케이스를 고민해봅니다.

### 주제 A: 다운샘플링(Downsampling)과 데이터 정확성
* **문제**: 다운샘플링 과정에서 데이터의 중요한 피크(Peak)나 특이점이 사라질 수 있습니다. (예: 1시간 평균 CPU는 30%지만, 순간적으로 100%를 찍었던 정보 소실)
* **접근법**:
    * 평균(avg)만 저장하는 것이 아니라, 최대(max), 최소(min), 합계(sum), 개수(count) 등을 함께 저장하여 조회 시 더 유연하게 분석할 수 있도록 합니다.

### 주제 B: 분산 시스템 추적(Distributed Tracing)과의 통합
* **문제**: "CPU 사용률이 90%다" (지표)를 넘어, "어떤 요청(Trace) 때문에 CPU 사용률이 높아졌는가?"를 알고 싶다면?
* **접근법**:
    * 지표 데이터에 Trace ID나 Span ID를 레이블로 추가하여 지표와 트레이스 데이터를 연결합니다.
    * 이를 통해 특정 지표 급증 시 관련된 요청의 상세 흐름을 바로 추적할 수 있습니다.

### 주제 C: 서비스 수준 목표(SLO) 기반 경보
* **문제**: "CPU 사용률 80% 초과" 같은 단순 임계값 경보는 실제 사용자 경험 저하와 직결되지 않을 수 있습니다.
* **접근법**:
    * **SLO(Service-Level Objective)**, 예를 들어 "99.9%의 요청이 200ms 안에 처리되어야 한다"를 정의합니다.
    * 이 SLO를 위반할 위험이 있을 때(Error Budget 소진 속도 기반) 경보를 발생시켜, 더 의미 있는 알림을 받도록 합니다.

---

## 5. 추가 학습 추천
* **Prometheus & Grafana**: 가장 널리 쓰이는 오픈소스 모니터링 및 시각화 도구. 직접 설치하고 사용해보는 것을 추천.
* **OpenTelemetry**: 지표, 로그, 트레이스를 통합 수집하기 위한 표준화된 프레임워크.
* **SRE (Site Reliability Engineering)**: 구글의 안정성 엔지니어링 문화. SLO, Error Budget 등 고급 경보 철학을 배울 수 있음.
